{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe84762-d0e5-4afd-95a1-5a0360f8f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number of inputs (x1,x2,x3..):  4\n",
      "Enter 4 input values:  1 -1 0 1\n",
      "Enter number of layers (hidden + output):  2\n",
      "Enter number of neurons in layer 1:  3\n",
      "Enter number of neurons in layer 2:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter weights for layer 1 (3 x 4):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0.2 0.3 0.5 0.4\n",
      " -0.1 0.2 -0.3 0.6\n",
      " 0.2 0.3 -0.1 0.4\n",
      "Enter 3 biases weight for layer 1:  0.2 0.3 0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter weights for layer 2 (1 x 3):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0.2 0.3 0.5\n",
      "Enter 1 biases weight for layer 2:  0.2\n",
      "\n",
      "Enter target output:  1\n",
      "Enter learning rate:  0.61\n",
      "Select activation function (sigmoid/tanh/relu/swish/softmax):  relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Output = 0.9299999999999999, Error = 0.07000000000000006\n",
      "Epoch 2: Output = 1.1035620068000003, Error = -0.10356200680000027\n",
      "Epoch 3: Output = 0.8355421053128729, Error = 0.16445789468712713\n",
      "Epoch 4: Output = 1.2276429608821646, Error = -0.22764296088216462\n",
      "Epoch 5: Output = 0.6335494244834519, Error = 0.36645057551654814\n",
      "Epoch 6: Output = 1.4009050956262006, Error = -0.4009050956262006\n",
      "Epoch 7: Output = 0.38882337090507524, Error = 0.6111766290949248\n",
      "Epoch 8: Output = 1.3176515298839633, Error = -0.3176515298839633\n",
      "Epoch 9: Output = 0.608247844785874, Error = 0.391752155214126\n",
      "Epoch 10: Output = 1.213885052129304, Error = -0.2138850521293041\n",
      "Epoch 11: Output = 0.7766325433958969, Error = 0.22336745660410307\n",
      "Epoch 12: Output = 1.1415824451039098, Error = -0.14158244510390983\n",
      "Epoch 13: Output = 0.8664955114450067, Error = 0.1335044885549933\n",
      "Epoch 14: Output = 1.0920953815413288, Error = -0.09209538154132879\n",
      "Epoch 15: Output = 0.9185553310791702, Error = 0.0814446689208298\n",
      "Epoch 16: Output = 1.059187857705909, Error = -0.059187857705909064\n",
      "Epoch 17: Output = 0.9497296761243218, Error = 0.0502703238756782\n",
      "Epoch 18: Output = 1.0377396160892731, Error = -0.03773961608927312\n",
      "Epoch 19: Output = 0.9687517714508938, Error = 0.03124822854910625\n",
      "Epoch 20: Output = 1.0239416454868255, Error = -0.023941645486825536\n",
      "Epoch 21: Output = 0.9804915136169894, Error = 0.019508486383010615\n",
      "Epoch 22: Output = 1.015139151010489, Error = -0.015139151010489016\n",
      "Epoch 23: Output = 0.987787871121885, Error = 0.012212128878114958\n",
      "Epoch 24: Output = 1.0095533323146717, Error = -0.009553332314671703\n",
      "Epoch 25: Output = 0.9923424667285103, Error = 0.007657533271489658\n",
      "Epoch 26: Output = 1.0060206406788705, Error = -0.006020640678870537\n",
      "Epoch 27: Output = 0.9951933440512499, Error = 0.0048066559487500715\n",
      "Epoch 28: Output = 1.0037911729732905, Error = -0.0037911729732904575\n",
      "Epoch 29: Output = 0.9969808585954214, Error = 0.003019141404578596\n",
      "Epoch 30: Output = 1.0023860499045538, Error = -0.002386049904553822\n",
      "Epoch 31: Output = 0.9981028418120097, Error = 0.0018971581879903\n",
      "Epoch 32: Output = 1.001501218038766, Error = -0.0015012180387659058\n",
      "Epoch 33: Output = 0.998807560144221, Error = 0.0011924398557789928\n",
      "Epoch 34: Output = 1.0009443192826173, Error = -0.0009443192826172897\n",
      "Epoch 35: Output = 0.999250381440533, Error = 0.0007496185594669802\n",
      "Epoch 36: Output = 1.0005939335229308, Error = -0.0005939335229308007\n",
      "Epoch 37: Output = 0.9995287094351987, Error = 0.0004712905648013299\n",
      "Epoch 38: Output = 1.000373526593965, Error = -0.0003735265939650567\n",
      "Epoch 39: Output = 0.9997036771262915, Error = 0.00029632287370850996\n",
      "Epoch 40: Output = 1.0002349000017092, Error = -0.0002349000017092262\n",
      "Epoch 41: Output = 0.9998136801086264, Error = 0.00018631989137363192\n",
      "Epoch 42: Output = 1.0001477170249622, Error = -0.00014771702496219064\n",
      "Epoch 43: Output = 0.9998828440519845, Error = 0.00011715594801553486\n",
      "Epoch 44: Output = 1.0000928900733324, Error = -9.289007333235233e-05\n",
      "Epoch 45: Output = 0.9999263324160875, Error = 7.366758391247341e-05\n",
      "Epoch 46: Output = 1.0000584120621274, Error = -5.8412062127422004e-05\n",
      "Epoch 47: Output = 0.9999536774041283, Error = 4.632259587167287e-05\n",
      "Epoch 48: Output = 1.0000367309617206, Error = -3.6730961720632394e-05\n",
      "Epoch 49: Output = 0.9999708719035207, Error = 2.9128096479258403e-05\n",
      "Epoch 50: Output = 1.0000230972288593, Error = -2.309722885929233e-05\n",
      "Epoch 51: Output = 0.9999816838980906, Error = 1.831610190938804e-05\n",
      "Epoch 52: Output = 1.0000145239947362, Error = -1.4523994736226342e-05\n",
      "Epoch 53: Output = 0.9999884825831196, Error = 1.1517416880435505e-05\n",
      "Epoch 54: Output = 1.0000091329572522, Error = -9.132957252244722e-06\n",
      "Epoch 55: Output = 0.9999927576783891, Error = 7.242321610867464e-06\n",
      "Epoch 56: Output = 1.000005742965722, Error = -5.742965722088655e-06\n",
      "Epoch 57: Output = 0.9999954459168363, Error = 4.554083163688105e-06\n",
      "Epoch 58: Output = 1.0000036112760033, Error = -3.6112760033457647e-06\n",
      "Epoch 59: Output = 0.999997136320713, Error = 2.8636792870351258e-06\n",
      "Epoch 60: Output = 1.0000022708315803, Error = -2.270831580331034e-06\n",
      "Epoch 61: Output = 0.9999981992726137, Error = 1.8007273863140938e-06\n",
      "Epoch 62: Output = 1.000001427936956, Error = -1.4279369560110666e-06\n",
      "Epoch 63: Output = 0.9999988676735089, Error = 1.1323264911267117e-06\n",
      "Epoch 64: Output = 1.000000897910514, Error = -8.979105139417243e-07\n",
      "\n",
      "Training complete: Error is effectively zero.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # stability trick\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "# Softmax derivative is usually handled with cross-entropy loss, not MSE\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "def swish_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s + x * s * (1 - s)\n",
    "# --- User input ---\n",
    "n_inputs = int(input(\"Enter number of inputs (x1,x2,x3..): \"))\n",
    "X = np.array(list(map(float, input(f\"Enter {n_inputs} input values: \").split())))\n",
    "\n",
    "n_layers = int(input(\"Enter number of layers (hidden + output): \"))\n",
    "layer_sizes = []\n",
    "for i in range(n_layers):\n",
    "    layer_sizes.append(int(input(f\"Enter number of neurons in layer {i+1}: \")))\n",
    "weights = []\n",
    "biases = []\n",
    "prev_size = n_inputs\n",
    "for i, size in enumerate(layer_sizes):\n",
    "    print(f\"\\nEnter weights for layer {i+1} ({size} x {prev_size}):\")\n",
    "    W = np.array([list(map(float, input().split())) for _ in range(size)])\n",
    "    b = np.array(list(map(float, input(f\"Enter {size} biases weight for layer {i+1}: \").split())))\n",
    "    weights.append(W)\n",
    "    biases.append(b)\n",
    "    prev_size = size\n",
    "target = float(input(\"\\nEnter target output: \"))\n",
    "lr = float(input(\"Enter learning rate: \"))\n",
    "activation_choice = input(\"Select activation function (sigmoid/tanh/relu/swish/softmax): \").lower()\n",
    "if activation_choice == \"sigmoid\":\n",
    "    activation, activation_derivative = sigmoid, sigmoid_derivative\n",
    "elif activation_choice == \"tanh\":\n",
    "    activation, activation_derivative = tanh, tanh_derivative\n",
    "elif activation_choice == \"relu\":\n",
    "    activation, activation_derivative = relu, relu_derivative\n",
    "elif activation_choice == \"swish\":\n",
    "    activation, activation_derivative = swish, swish_derivative\n",
    "elif activation_choice == \"softmax\":\n",
    "    activation, activation_derivative = softmax, None\n",
    "else:\n",
    "    raise ValueError(\"Unsupported activation function selected.\")\n",
    "epoch = 1\n",
    "error = float('inf')\n",
    "max_epochs = 10000\n",
    "while abs(error) > 1e-6 and epoch <= max_epochs:\n",
    "    activations = [X]\n",
    "    nets = []\n",
    "    for i in range(len(weights)):\n",
    "        net = np.dot(weights[i], activations[-1]) + biases[i]\n",
    "        nets.append(net)\n",
    "        activations.append(activation(net))\n",
    "    output = activations[-1][0] if activation_choice != \"softmax\" else activations[-1]\n",
    "    error = target - output if activation_choice != \"softmax\" else (target - output[0])\n",
    "    # Backpropagation (skip softmax derivative for simplicity)\n",
    "    if activation_choice != \"softmax\":\n",
    "        deltas = [error * activation_derivative(nets[-1])]\n",
    "        for i in range(len(weights) - 2, -1, -1):\n",
    "            delta = np.dot(weights[i+1].T, deltas[-1]) * activation_derivative(nets[i])\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] += lr * np.outer(deltas[i], activations[i])\n",
    "            biases[i] += lr * deltas[i]\n",
    "\n",
    "    print(f\"Epoch {epoch}: Output = {output}, Error = {error}\")\n",
    "    epoch += 1\n",
    "\n",
    "if epoch > max_epochs:\n",
    "    print(\"\\nReached maximum epochs without reaching exact target.\")\n",
    "else:\n",
    "    print(\"\\nTraining complete: Error is effectively zero.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ffb87-b683-4ad5-ac7e-4f8c5bd636c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
